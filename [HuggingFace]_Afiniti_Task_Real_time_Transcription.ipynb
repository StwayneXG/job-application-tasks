{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJJLbqcUr7cb+NKmkcost+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StwayneXG/job-application-tasks/blob/main/%5BHuggingFace%5D_Afiniti_Task_Real_time_Transcription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3sBOhArVE77"
      },
      "outputs": [],
      "source": [
        "#@title Install Dependencies\n",
        "# Download these to:\n",
        "# 1. Load models\n",
        "# 2. Use microphone (cannot use mic on colab)\n",
        "# 3. Load evaluation functions\n",
        "\n",
        "# Only for Linux based systems\n",
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg > /dev/null \n",
        "\n",
        "!pip install transformers > /dev/null\n",
        "!pip install sounddevice > /dev/null\n",
        "!pip install jiwer > /dev/null\n",
        "!wget -O speech.wav https://github.com/EN10/DeepSpeech/blob/master/man1_wb.wav?raw=true"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Model\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
        "model.config.forced_decoder_ids = None"
      ],
      "metadata": {
        "id": "xCd1XQ_SVJKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Audio Parameters\n",
        "\n",
        "sample_rate = 16000             # Standard + the model we used was trained on 16 KHz Sampling Rate\n",
        "chunk_size = 32*1024            # Require a decent chunk of the audio to get context and avoid getting half words"
      ],
      "metadata": {
        "id": "lvGYCntuWb49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set up Input Stream\n",
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "\n",
        "use_mic = False                           # Set to True to test on PC and use mic\n",
        "predicted_transcript = \"\"                 # Will contain complete transcription. Stored to be later used as context by question-answering model\n",
        "\n",
        "if use_mic:\n",
        "    has_audio_input = True\n",
        "    def transcript_audio(indata, frames, time, status):                         # Callback function after recording each chunk\n",
        "        if status:\n",
        "            print('Error:', status)\n",
        "\n",
        "        global has_audio_input\n",
        "        global predicted_transcript\n",
        "        has_audio_input = np.max(indata) > 0                                    # To stop if no audio is heard\n",
        "\n",
        "        indata = indata.astype(np.float64) / np.iinfo(np.int16).max             # Model we're using requires audio signal in Float64 dtype\n",
        "\n",
        "        # Extract Features\n",
        "        input_features = processor(np.array(indata, dtype=\"float64\"), \n",
        "                                   sampling_rate=sample_rate, \n",
        "                                   return_tensors=\"pt\").input_features\n",
        "        # Generate Output\n",
        "        predicted_ids = model.generate(input_features)\n",
        "        # Decode to get text\n",
        "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Store transcription\n",
        "        predicted_transcript = f\"{predicted_transcript} {transcription}\"\n",
        "        print(transcription)\n",
        "\n",
        "    # Stop with Ctrl + C after speaking the sentence given below. (if it doesn't automatically stop)\n",
        "    ground_truth = r\"\"\" In a bustling city, a young artist found solace in her attic studio, \n",
        "                        expressing her imagination through vibrant art. Her creations became \n",
        "                        a sanctuary, touching hearts and inspiring others. Amidst the chaos, \n",
        "                        she found peace and purpose in her creative haven.\"\"\".lower()\n",
        "\n",
        "    # Input audio using the microphone stream\n",
        "    with sd.InputStream(callback=transcript_audio, channels=1, samplerate=sample_rate, blocksize=chunk_size, dtype='int16'):\n",
        "        while True:\n",
        "            if has_audio_input:\n",
        "                continue\n",
        "            print(\"stream ended\")\n",
        "            break\n",
        "\n",
        "else:\n",
        "    # To test on colab, I used a wave file\n",
        "    import wave\n",
        "    ground_truth = \"in the course of a december tour in yorkshire i rode for a long distance in one of the public coaches on the day preceding christmas\"\n",
        "    \n",
        "    # Open file, load audio signal and get Sampling Rate\n",
        "    wav_file = wave.open('speech.wav', 'rb') \n",
        "    sample_rate = wav_file.getframerate()\n",
        "    \n",
        "    while True:\n",
        "        # Read chunk size (simulating our requirement of infinite stream)\n",
        "        frames = wav_file.readframes(chunk_size)\n",
        "        if not frames:\n",
        "            break\n",
        "\n",
        "        # Get data and convert to Float64\n",
        "        indata = np.frombuffer(frames, dtype=np.int16)\n",
        "        indata = indata.astype(np.float64) / np.iinfo(np.int16).max\n",
        "\n",
        "        # Process and Decode\n",
        "        input_features = processor(np.array(indata, dtype=\"float64\"), sampling_rate=sample_rate, return_tensors=\"pt\").input_features \n",
        "        predicted_ids = model.generate(input_features)\n",
        "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0].lower()\n",
        "\n",
        "        predicted_transcript += transcription\n",
        "        print(transcription)\n",
        "print(\"Complete transcription: \\n\" + predicted_transcript)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V-dYz97V-yp",
        "outputId": "e9ab45c6-79b3-4040-a3fe-bc978a8b3c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " in the course of a december tour in york\n",
            " i rode for a long distance.\n",
            " in one of the public coaches on the day.\n",
            " day preceding christmas.\n",
            " you\n",
            "Complete transcription: \n",
            " in the course of a december tour in york i rode for a long distance. in one of the public coaches on the day. day preceding christmas. you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluation\n",
        "\n",
        "import jiwer\n",
        "\n",
        "# Evaluate for each error type\n",
        "wer_error = jiwer.wer(ground_truth, predicted_transcript)\n",
        "mer_error = jiwer.mer(ground_truth, predicted_transcript)\n",
        "wil_error = jiwer.wil(ground_truth, predicted_transcript)\n",
        "\n",
        "print(f\"Word Error Rate (WER): {wer_error}\")          # It calculates the percentage of word-level errors between the two.\n",
        "print(f\"Match Error Rate (MER): {mer_error}\")         # It considers both insertions (extra words) and deletions (missing words) in addition to substitutions\n",
        "print(f\"Word Information Lost (WIL): {wil_error}\")    # It calculates the percentage of words in the reference transcription that were not present in the output transcription."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2gf0LQYmpus",
        "outputId": "16888b04-e30f-413f-cb59-8ee1d39e565e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Error Rate (WER): 0.19230769230769232\n",
            "Match Error Rate (MER): 0.17857142857142858\n",
            "Word Information Lost (WIL): 0.2733516483516484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Question Answering\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load question-answering model from HuggingFace\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "\n",
        "# Question\n",
        "question = \"when is the story taking place?\"\n",
        "\n",
        "# Pass in question along with context\n",
        "inputs = tokenizer(question, predicted_transcript, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Identify start and end index (for answer)\n",
        "answer_start_index = torch.argmax(outputs.start_logits)\n",
        "answer_end_index = torch.argmax(outputs.end_logits)\n",
        "\n",
        "# Get indexes and decode\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CIcP3mgQnSeG",
        "outputId": "97e05e3a-e4dc-43cf-8127-9e0c8adde8f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'day preceding christmas'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    }
  ]
}